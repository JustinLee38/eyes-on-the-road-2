{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsJybSKZmLHA"
   },
   "source": [
    "## 1. Transformer & Attention\n",
    "\n",
    "--------------------------------------\n",
    "**_Disclaimer: I would like to spend some time describing the attention mechanism because for implementation of Vision Transformer I will use [Keras MultieadAttention layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). So what goes on there is worth understanding. If you want to jump into vision transformer implementation jump to section 2._**   \n",
    "\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "\n",
    "To understand Vision Transformer, first we need to focus on the basics of transformer and attention mechanism. For this part I will follow the paper **[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)**. This paper itself is an excellent read and the description/concepts below are mostly taken from there and it will only help us to proceed further. \n",
    "\n",
    "\n",
    "\n",
    " The idea of transformer is to use attention without recurrence (read RNN). So transformer is still a sequence to sequence (Seq2Seq) model and follows encoder-decoder structure. To quote from the paper--\n",
    " \n",
    ">The encoder maps an input sequence of symbol representations $(x_1, \\ldots, x_n)$ to a sequence\n",
    "of continuous representations $z = (z_1, \\ldots, z_n)$. Given $z$, the decoder then generates an output sequence $(y_1, \\ldots, y_m)$ of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. \n",
    "\n",
    "\n",
    "Let's see the Transformer structure introduced in the paper\n",
    "\n",
    "![Transformer](https://drive.google.com/uc?id=1BXHTi4lUivqVv-T7VnTWgEdQ-2pBVUfY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVLQPFwap4Pz"
   },
   "source": [
    "The encoder takes the input data (sentence/sequence), and produces an intermediate representation of the input. The decoder decodes this intermediate representation step by step and generates the output. The $Nx$ part in above picture are number of stacked layers, in orginal paper it's 6.  Once we understand the encoder part of the above structure we can move to vision transformer. Encoder layer contains 2 very important components, \n",
    "\n",
    "1. Multi-head self-attention block. \n",
    "2. Position wise, fully-connected feed forward network.   \n",
    "\n",
    "Let's focus on the multi-head self attention part. The paper itself has a diagram of scaled dot-product attention and multi-head attention which consists of several attention layers running in parallel. \n",
    "\n",
    "![mutli-head](https://drive.google.com/uc?id=1_dXyDo1khaK4pmcfFzFyOZhDnSUL5Rux)\n",
    "\n",
    "The 3 labels in the diagram Q, K, V denotes Query, Key and Value vectors. For now we think of this as part of information retrieval protocol when we search (query) and the search engine compares our query with a key and responds with a value (output).\n",
    "\n",
    "In the orginal paper 3 different usages of multi-head attention were described. Let's quote directly from paper-\n",
    "\n",
    "> 1. In \"encoder-decoder attention\" layers, the _queries_ come from the previous decoder layer, and the memory _keys_ and _values_ come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models. \n",
    "2. The encoder contains self-attention layers. In a self-attention layer all of the _keys_, _values_ and _queries_ come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. \n",
    "3. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
    "\n",
    "For our purpose (to understand vision transformer), most important point is 2, i.e. self-attention in encoder part. Let's deep dive! \n",
    "\n",
    "\n",
    "## 1.1. Self-Attention\n",
    "\n",
    "Let's consider an input sequence $(x_1, x_2, \\ldots , x_m)$. Output of self-attention layer from this input sequence is a set of context vectors of same length $(C_1, C_2, \\ldots , C_m)$ as input sequence. The picture below will help us\n",
    "\n",
    "![S-attention1](https://drive.google.com/uc?id=1YNZcH34tH4LK4obSEyDyyDRobbD6tddV)\n",
    "\n",
    "In the picture above we define the weights that will be trained as $W_q, W_k, W_v$ for _query_, _keys_ and _values_. How they are actually used? Let's see the picture below--\n",
    "\n",
    "![S-attention2](https://drive.google.com/uc?id=1WHEc75C4TKpc4VBiwNPO1XqbWo9JvQt6)\n",
    "\n",
    "It is important to note that same weight is applied for all $i$s. In the _Attention is All You Need_ paper, the dimensions of _query_ and _key_ are taken as $d_k$ and for _values_ it is assumed $d_v$. For example if we have  $x_i$s of dimension 5 i.e. [0 1 1 2 3] and _query_ has dimension 3 then $W_q$ will have dimension $5\\times 3$. Same goes for _keys_ and _values_ and corresponding weights. So how these context vectors are calculated? Let's see below--\n",
    "\n",
    "![S-attention3](https://drive.google.com/uc?id=1ca3Eu4LMAbKqg6IxQDEfmTJVrsEdCxHG)\n",
    "\n",
    "The figure above describes the dot-product attention which was introduced in the paper. Important point is for calculating $\\alpha _j$ (see figure above) at position $q_j$ (same as $x_j$) we use information from $q_j$ only but all other _Keys_ ($k_j$s). We are left with the final step to calculate the output of attention layer and that is to use the _values_ as below--\n",
    "\n",
    "![S-attention4](https://drive.google.com/uc?id=14AYcINN08Af983BcyGBeOT7eaOM278Lx)\n",
    "\n",
    "Here we see how we take the product of $v_i$ with $\\alpha \\equiv Softmax \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$.  \n",
    "\n",
    "To calculate context vector at position $i$ we need values from all the inputs. Finally, how do we think about the scaling factor $\\left(\\frac{1}{\\sqrt{d_k}} \\right)$ in the scaled dot attention function ?\n",
    "\n",
    "Usually we will initialize our layers with the intention of having equal variance throughout the model. But when we perform the dot product over these two vectors $(Q, K)$ with a variance $\\sigma ^2$, this will result in a scaler having $d_k$ times higher variance. Remember also that $d_k$ is the dimension of both $Q, K$, while $V$ has dimension $d_v$. \n",
    "\n",
    "$$q_i \\sim N(0, \\sigma ^2), \\, k_i \\sim N(0, \\sigma ^2), \\rightarrow \\text{Var} \\left(\\sum \\limits_{i=1}^{d_k} q_i \\cdot k_i \\right) = \\sigma ^2 \\cdot d_k $$ \n",
    "\n",
    "If we do not scale down the variance back to $\\sigma ^2$, the softmax over the logits would have saturated to $1$ for one random element and $0$ for all others. The gradients through the softmax will be close to zero so that we can’t learn the parameters appropriately. \n",
    "\n",
    "At this point we can at-least appreciate the scaled dot product attention diagram (figure: 2) that was introduced in the paper. \n",
    "\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "## 1.2. Multi-Head Attention (Self)\n",
    "\n",
    "It's a very simple extension of single head self-attention. \n",
    "\n",
    "1. In Multi-head self-attention we have $h$ single head self-attentions (layers). \n",
    "\n",
    "2. In a single head self attention trainable parameters are weights $W_q, W_k, W_v$\n",
    "\n",
    "3. The $h$ single head self-attention layers do not share parameters. So total $3h$ parameters. \n",
    "\n",
    "4. Each single head self-attention outputs a context vector. \n",
    "\n",
    "5. These context vectors are concatenated. \n",
    "\n",
    "6. If a single head attention outputs a dimensional vector i.e. each $C_i$’s are $d \\times 1$, then multi-head outputs are $hd \\times 1$ dimensional vector, given $h$ layers of single head self-attention layers.      \n",
    "\n",
    "To quote from paper on the importance of multi-head attention--\n",
    "\n",
    "> Multi-head attention allows the model to jointly attend to information from different representation subspaces at different position. With a single attention head, averaging inhibits this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JmhmSb6gEee"
   },
   "source": [
    "\n",
    "## 2. Vision Transformer (Step by Step)\n",
    "\n",
    "Implement vision transformer on [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "For this implementation we will use concepts and methods described in the original [ViT paper](https://arxiv.org/abs/2010.11929) by Dosovitskiy et.al. Here it is mentioned that ViTs are data hungry architectures and the performance of ViTs even in relatively large dataset like ImageNet without strong regularization yields accuracies few percentages below ResNet. But the scenario changes when Transformers are trained on larget datasets (14M-300M images). The original implementation is available in [google github](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models.py) and a very similar version in [tensorflow models](https://github.com/tensorflow/models/blob/master/official/vision/beta/projects/vit/modeling/vit.py). We will discuss each step in detail but first let's get started with loading data. \n",
    "\n",
    "**ViT TLDR:** \n",
    "\n",
    "1. Take an image $(256 \\times 256 \\times 3)$.\n",
    "2. Turn images into smaller patches $\\left(\\text{ex:}\\,  16 \\times 16 \\times 3 \\, ,\\,  \\, \\text{total} \\, 256\\,  \\left(N=\\frac{256 \\times 256}{16^2} \\right)  \\text{patches} \\right)$. \n",
    "3. These patches then were linearly embedded. We can think of these now as tokens. \n",
    "4. Use them as input for Transformer Encoder (contains multi-head self attention). \n",
    "5. Perform the classification. \n",
    "6. Bye Bye Convolution. \n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JK89R2IFatAN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24768,
     "status": "ok",
     "timestamp": 1644144672629,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "OB3ZHu48aItN",
    "outputId": "1fcfa95f-2c4f-402f-e15d-cf46498f9f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 23s 0us/step\n",
      "check shapes:  (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n",
      "train data shape after the split:  (40000, 32, 32, 3)\n",
      "new validation data shape:  (10000, 32, 32, 3)\n",
      "validation labels shape:  (10000, 10)\n",
      "train im and label types:  <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "check types;  <class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'> <class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print ('check shapes: ', x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# train_im, test_im = x_train/255.0 , x_test/255.0\n",
    "\n",
    "train_lab_categorical = tf.keras.utils.to_categorical(y_train, num_classes=10, dtype='uint8')\n",
    "\n",
    "test_lab_categorical = tf.keras.utils.to_categorical(y_test, num_classes=10, dtype='uint8')\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "train_im, valid_im, train_lab, valid_lab = train_test_split(x_train, train_lab_categorical, test_size=0.20, \n",
    "                                                            stratify=train_lab_categorical, \n",
    "                                                            random_state=40, shuffle = True) # stratify is unncessary \n",
    "\n",
    "print (\"train data shape after the split: \", train_im.shape)\n",
    "print ('new validation data shape: ', valid_im.shape)\n",
    "print (\"validation labels shape: \", valid_lab.shape)\n",
    "\n",
    "class_types = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck'] # from cifar-10 website\n",
    "\n",
    "print ('train im and label types: ', type(train_im), type(train_lab))\n",
    "\n",
    "training_data = tf.data.Dataset.from_tensor_slices((train_im, train_lab))\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((valid_im, valid_lab))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, test_lab_categorical))\n",
    "\n",
    "print ('check types; ', type(training_data), type(validation_data))\n",
    "\n",
    "\n",
    "autotune = tf.data.AUTOTUNE \n",
    "\n",
    "train_data_batches = training_data.shuffle(buffer_size=40000).batch(128).prefetch(buffer_size=autotune)\n",
    "valid_data_batches = validation_data.shuffle(buffer_size=10000).batch(32).prefetch(buffer_size=autotune)\n",
    "test_data_batches = test_data.shuffle(buffer_size=10000).batch(32).prefetch(buffer_size=autotune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y8ijSW_b6Wj"
   },
   "source": [
    "## 2.1. Patch Generation \n",
    "\n",
    "Let's discuss what's described in paper. \n",
    "\n",
    "Consider an image, $x \\in \\mathbb{R}^{H\\times W \\times C}$, and turn it into a sequence of patches $x_p \\in \\mathbb{R}^{N\\times P \\times P \\times C}$, where $(H, W )$ is the height and Width of the original image, $C$ is the number of channels, $(P, P )$ is the resolution of each image patch, and $N = HW/P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.   \n",
    "\n",
    "Check the tensorflow example page about [custom layer](https://www.tensorflow.org/tutorials/customization/custom_layers) for examples, we follow the example given there. \n",
    "\n",
    "The best way to implement your own layer is extending the tf.keras.Layer class and implementing:\n",
    "\n",
    "1. `__init__` , where you can do all input-independent initialization.\n",
    "2.     `build`, where you know the shapes of the input tensors and can do the rest of the initialization, *this part isn't necessary*.\n",
    "3. `call`, where you do the forward computation.\n",
    "\n",
    "\n",
    "For the patch generation, I will follow what was done in [original code](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models.py) but I will discuss another method too which was discussed in [Keras Blog](https://keras.io/examples/vision/image_classification_with_vision_transformer/).  \n",
    "\n",
    "1. We can use [`tf.image.extract_patches`](https://www.tensorflow.org/api_docs/python/tf/image/extract_patches). To quote from the page\n",
    "\n",
    ">This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output.\n",
    "\n",
    "Using this we can literally create patches from the images, the patches were then flattened. Then use a dense layer with learnable weights to project it with a hidden dimension (this will be more clear soon). In addition, it adds a learnable position embedding to the projected vector.  Final shape of the output will be `(batch_size, num_patches, hidden_dim)`. An example using an image tensor till patch creation is shown below. \n",
    "\n",
    "2. In the original code instead of creating patches and then adding learnable weights via Dense layer to project it on a certain dimension, we directly use a convolutional layer (with learnable weights) with number of filters equal to this hidden dimension. So the shape here is already `(batch_size, num_patches, hidden_dim)` and then a learnable position embedding layer of same shape was added to the input. \n",
    "\n",
    "We will discuss both methods. But before that what's this hidden dimension ? This is the dimension of Query and Key (previously we wrote it as $d_k$) and we will use this in the encoder block when we need the [`MultiHeadAttention` layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). So this projection is done in such a way that we can directly feed the embedded patches (flattened) to the transformer. Great ! Slowly things are building up. \n",
    "\n",
    "\n",
    "### 2.1.1 Patch Generation to Embed (Keras Blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cMA-vXAvHsva"
   },
   "outputs": [],
   "source": [
    "class generate_patch(layers.Layer):\n",
    "  def __init__(self, patch_size):\n",
    "    super(generate_patch, self).__init__()\n",
    "    self.patch_size = patch_size\n",
    "    \n",
    "  def call(self, images):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(images=images, \n",
    "                                       sizes=[1, self.patch_size, self.patch_size, 1], \n",
    "                                       strides=[1, self.patch_size, self.patch_size, 1], rates=[1, 1, 1, 1], padding=\"VALID\")\n",
    "    patch_dims = patches.shape[-1]\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dims]) #here shape is (batch_size, num_patches, patch_h*patch_w*c) \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786
    },
    "executionInfo": {
     "elapsed": 5576,
     "status": "ok",
     "timestamp": 1644144681323,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "6ZsNcBTdhnu5",
    "outputId": "80a576d0-9d1b-4e88-a127-57ab32b8400f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check shapes:  (1, 32, 32, 3)\n",
      "patch per image and patches shape:  64 \n",
      " (1, 64, 48)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 64 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import islice, count\n",
    "\n",
    "train_iter_7im, train_iter_7label = next(islice(training_data, 7, None)) # access the 7th element from the iterator\n",
    "\n",
    "\n",
    "\n",
    "train_iter_7im = tf.expand_dims(train_iter_7im, 0)\n",
    "train_iter_7label = train_iter_7label.numpy()\n",
    "\n",
    "print('check shapes: ', train_iter_7im.shape) \n",
    "\n",
    "patch_size=4 \n",
    "######################\n",
    "# num patches (W * H) /P^2 where W, H are from original image, P is patch dim. \n",
    "# Original image (H * W * C), patch N * P*P *C, N num patches\n",
    "######################\n",
    "generate_patch_layer = generate_patch(patch_size=patch_size)\n",
    "patches = generate_patch_layer(train_iter_7im)\n",
    "\n",
    "print ('patch per image and patches shape: ', patches.shape[1], '\\n', patches.shape)\n",
    "\n",
    "\n",
    "\n",
    "def render_image_and_patches(image, patches):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(tf.cast(image[0], tf.uint8))\n",
    "    plt.xlabel(class_types [np.argmax(train_iter_7label)], fontsize=13)\n",
    "    n = int(np.sqrt(patches.shape[1]))\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    #plt.suptitle(f\"Image Patches\", size=13)\n",
    "    for i, patch in enumerate(patches[0]):\n",
    "        ax = plt.subplot(n, n, i+1)\n",
    "        patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "        ax.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "        ax.axis('off')    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "render_image_and_patches(train_iter_7im, patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5_9D7tkOjD1c"
   },
   "outputs": [],
   "source": [
    "### Positonal Encoding Layer\n",
    "\n",
    "class PatchEncode_Embed(layers.Layer):\n",
    "  '''\n",
    "  2 steps happen here\n",
    "  1. flatten the patches \n",
    "  2. Map to dim D; patch embeddings  \n",
    "  '''\n",
    "  def __init__(self, num_patches, projection_dim):\n",
    "    super(PatchEncode_Embed, self).__init__()\n",
    "    self.num_patches = num_patches\n",
    "    self.projection = layers.Dense(units=projection_dim)# activation = linear\n",
    "    self.position_embedding = layers.Embedding(\n",
    "        input_dim=num_patches, output_dim=projection_dim)\n",
    "    \n",
    "  def call(self, patch):\n",
    "    positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "    encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1603,
     "status": "ok",
     "timestamp": 1644144682915,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "v6LAY5gHzAFm",
    "outputId": "584719b7-f68e-477a-bee3-70a91ec47d10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 64 64], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "patch_encoder = PatchEncode_Embed(64, 64)(patches)\n",
    "print (tf.shape(patch_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f9hT0LJG1EUI"
   },
   "outputs": [],
   "source": [
    "### compare with conv layer (tf extract image)\n",
    "# train_iter_7im = tf.cast(train_iter_7im, dtype=tf.float32)\n",
    "# x_conv = layers.Conv2D(48, patch_size, patch_size, padding='valid')(train_iter_7im)\n",
    "\n",
    "# print (x_conv.shape)\n",
    "\n",
    "\n",
    "# patches = tf.image.extract_patches(images=train_iter_7im, \n",
    "#                                        sizes=[1, patch_size, patch_size, 1], \n",
    "#                                        strides=[1, patch_size, patch_size, 1], rates=[1, 1, 1, 1], padding=\"VALID\")\n",
    "\n",
    "# print (patches.shape)\n",
    "\n",
    "# patches = tf.reshape(patches, [1, -1, patches.shape[-1] ] )\n",
    "# print (patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGYSAScJx37_"
   },
   "source": [
    "### 2.2. Patches and Embedding (Original Implementation)\n",
    "\n",
    "How the original implementation works were described in section 2.1 (Patch Generation), point 2. \n",
    "\n",
    "Here I wrote 2 different layers, \n",
    "\n",
    "1. `generate_patch_conv`: Directly compare with the extract image part we checked before, to understand conv layer is very similar to `extract image` method except of course `Conv2D` layer is leranable. \n",
    "\n",
    "1. `generate_patch_conv_orgPaper`: What was done by the google researchers. Number of filters in the `Conv2D` layer matches the hidden dimension i.e. query/key dimension in multi-head attention layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6474,
     "status": "ok",
     "timestamp": 1644144692653,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "N5rSItYUeAd4",
    "outputId": "f96a1c25-0e82-400d-9e9c-ccd153fab514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch per image and patches shape:  64 \n",
      " (1, 64, 48)\n",
      "patch per image and patches shape:  64 \n",
      " (1, 64, 64)\n",
      "patch per image and patches shape:  64 \n",
      " (1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "class generate_patch_conv(layers.Layer):\n",
    "  '''\n",
    "  this is an example to generate conv patches comparable with the image patches\n",
    "  generated using tf extract image patches. This wasn't the original implementation, specially \n",
    "  the number of filters in the conv layer has nothing to do with patch size. It must be same as\n",
    "  hidden dim (query/key dim) in relation to multi-head attention layer.       \n",
    "  '''\n",
    "  def __init__(self, patch_size):\n",
    "    super(generate_patch_conv, self).__init__()\n",
    "    self.patch_size = patch_size\n",
    "\n",
    "  def call(self, images):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = layers.Conv2D(self.patch_size*self.patch_size*3, self.patch_size, self.patch_size, padding='valid')(images)\n",
    "\n",
    "    patch_dims = patches.shape[-1]\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "    return patches  \n",
    "\n",
    "class generate_patch_conv_orgPaper(layers.Layer):\n",
    "  '''\n",
    "  original implementation, \n",
    "  hidden size = query/key dim in multi head attention layer later. \n",
    "  '''\n",
    "  def __init__(self, patch_size, hidden_size):\n",
    "    super(generate_patch_conv_orgPaper, self).__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "  def call(self, images):\n",
    "    patches = layers.Conv2D(self.hidden_size, self.patch_size, self.patch_size, padding='valid', name='Embedding')(images) \n",
    "    # kernels and strides = patch size\n",
    "    # the weights of the convolutional layer will be learned. \n",
    "    rows_axis, cols_axis = (1, 2) # channels last images\n",
    "    #if channels_last:\n",
    "      #rows_axis, cols_axis = (2, 3) # for channels last\n",
    "      # x = tf.transpose(patches, perm=[0, 2, 3, 1]) # do this for channels_first\n",
    "    seq_len = (images.shape[rows_axis] // patch_size) * (images.shape[cols_axis] // patch_size)\n",
    "    x = tf.reshape(patches, [-1, seq_len, self.hidden_size])\n",
    "    return x\n",
    "\n",
    "def generate_patch_conv_orgPaper_f(patch_size, hidden_size, inputs):\n",
    "  patches = layers.Conv2D(filters=hidden_size, kernel_size=patch_size, strides=patch_size, padding='valid')(inputs)\n",
    "  row_axis, col_axis = (1, 2) # channels last images\n",
    "  seq_len = (inputs.shape[row_axis] // patch_size) * (inputs.shape[col_axis] // patch_size)\n",
    "  x = tf.reshape(patches, [-1, seq_len, hidden_size])\n",
    "  return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_iter_7im = tf.cast(train_iter_7im, dtype=tf.float16)\n",
    "generate_patch_conv_layer = generate_patch_conv(patch_size=patch_size)\n",
    "patches_conv = generate_patch_conv_layer(train_iter_7im)\n",
    "\n",
    "\n",
    "print ('patch per image and patches shape: ', patches_conv.shape[1], '\\n', patches_conv.shape)    \n",
    "\n",
    "generate_patch_conv_orgPaper_layer = generate_patch_conv_orgPaper(patch_size=patch_size, hidden_size=64)\n",
    "patches_conv_org = generate_patch_conv_orgPaper_layer(train_iter_7im)\n",
    "\n",
    "hidden_size=64\n",
    "patches_conv_org_f = generate_patch_conv_orgPaper_f(patch_size, hidden_size, train_iter_7im)\n",
    "\n",
    "print ('patch per image and patches shape: ', patches_conv_org.shape[1], '\\n', patches_conv_org.shape)\n",
    "print ('patch per image and patches shape: ', patches_conv_org_f.shape[1], '\\n', patches_conv_org_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "executionInfo": {
     "elapsed": 3017,
     "status": "ok",
     "timestamp": 1644144695637,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "0_bjBJCti6yP",
    "outputId": "8d82ba6e-2701-4f6a-d60b-3e6651c52723"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 64 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render_image_and_patches(image, patches):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(tf.cast(image[0], tf.uint8))\n",
    "    plt.xlabel(class_types [np.argmax(train_iter_7label)], fontsize=13)\n",
    "    n = int(np.sqrt(patches.shape[1]))\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    #plt.suptitle(f\"Image Patches\", size=13)\n",
    "    for i, patch in enumerate(patches[0]):\n",
    "        ax = plt.subplot(n, n, i+1)\n",
    "        patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "        ax.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "        ax.axis('off')\n",
    "\n",
    "render_image_and_patches(train_iter_7im, patches_conv)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XwoVIVyCjaJs"
   },
   "outputs": [],
   "source": [
    "# x_conv = layers.Conv2D(64, patch_size, patch_size, padding='valid')(train_iter_7im)\n",
    "\n",
    "# print(x_conv.shape)\n",
    "\n",
    "# #x_conv = tf.transpose(x_conv, perm=[0, 2, 3, 1])\n",
    "\n",
    "# #print(x_conv.shape)\n",
    "\n",
    "# seq_len = (train_iter_7im.shape[1] // patch_size) * (train_iter_7im.shape[2] // patch_size)\n",
    "\n",
    "# x = tf.reshape(x_conv, [-1, seq_len, 64])\n",
    "\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1644144695638,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "49zwF0Cim9Nn",
    "outputId": "60058f63-c19a-4ba3-e49a-839f62f35f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded input shape:  (1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "### Positonal Encoding Layer\n",
    "\n",
    "class AddPositionEmbs(layers.Layer):\n",
    "  \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n",
    "\n",
    "  def __init__(self, posemb_init=None, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.posemb_init = posemb_init\n",
    "    #posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02), name='posembed_input') # used in original code\n",
    "\n",
    "  def build(self, inputs_shape):\n",
    "    pos_emb_shape = (1, inputs_shape[1], inputs_shape[2])\n",
    "    self.pos_embedding = self.add_weight('pos_embedding', pos_emb_shape, initializer=self.posemb_init)\n",
    "\n",
    "  def call(self, inputs, inputs_positions=None):\n",
    "    # inputs.shape is (batch_size, seq_len, emb_dim).\n",
    "    pos_embedding = tf.cast(self.pos_embedding, inputs.dtype)\n",
    "\n",
    "    return inputs + pos_embedding\n",
    "\n",
    "\n",
    "pos_embed_layer = AddPositionEmbs(posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02))\n",
    "embedded_inp = pos_embed_layer(patches_conv_org)\n",
    "\n",
    "print ('embedded input shape: ', embedded_inp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA8UIQRQiQEA"
   },
   "source": [
    "Let's discuss some important point related to the encoding of the patches. To quote from the paper--  \n",
    "\n",
    "> The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to\n",
    "the output of this projection as the patch embeddings. \n",
    "\n",
    "$$z_0 = \\left[x_{\\text{class}}; x_p^1\\cdot E; x_p^2\\cdot E; \\ldots x_p^N\\cdot E \\right] + E_{\\text{pos}}; \\, \\, E \\in \\mathbb{R}^{(P^2 \\times C)\\times D} , E_{\\text{pos}} \\in \\mathbb{R}^{(N +1)\\times D} \\, \\ldots (1)$$\n",
    "\n",
    "The `cls` token is similar to that of [BERT's](https://arxiv.org/abs/1810.04805) class token. \n",
    "\n",
    "How a ViT architecture is different from a CNN and Positional Embedding--\n",
    "\n",
    "Once we flatten the images to patches, we lose the structures of the input and the positional embedding helps allow the model to learn about the structure of the input image. These positional embeddings are learnable and highlights how much about image structure the model can learn on its own. In CNN if we consider kernels, it's about learning 2-D neighborhood structure; but in transformers apart from the MLP layer this local 2-D structure isn't used and the positional embeddings at the initialization time carry no information about the 2-D position of the patches and all spatial relations between the patches are learned from scratch.       \n",
    "\n",
    "\n",
    "Afer the embedding part, we feed them into the multi-head self attention and multi-layer perceptron layers (in that order), and that essentially is the transforner block in VIT. A visual representation is take from the [Google blog](https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html) is shown below.   \n",
    "\n",
    "![VITransformer_Gif](https://drive.google.com/uc?id=1BUBJ7pSJxNVIrhkpex6owgrjixXiwpAl)\n",
    "\n",
    "\n",
    "The visual representation of specifically  the Transformer encoder block is shown below and this will help us to implement it better. \n",
    "\n",
    "![Transf_Enc](https://drive.google.com/uc?id=1uKWAIAHjwbP_jhSj_AR6UC7LdawDizOn)\n",
    "\n",
    "\n",
    "\n",
    "## 2.3. Transformer Encoder with Multi-Head Self Attention \n",
    "\n",
    "Here we will implement the Transformer Encoder block shown above. \n",
    "\n",
    "Few Points ---\n",
    "\n",
    "1. We still need to define the Multilayer Perceptron (MLP) part. \n",
    "2. What is `Norm` ? How to implement? \n",
    "3. We can clearly see 2 residual connections, which will also be added. \n",
    "\n",
    "\n",
    "**_MLP:_** . To quote from the paper: \n",
    "\n",
    "> The MLP contains two layers with a GELU non-linearity. \n",
    "\n",
    "Check out the Gaussian Error Linear Units [(GELU) paper](https://arxiv.org/abs/1606.08415) and I skip the discussion here. This activation is available within `tf.nn`. The MLP sizes are given in paper and for this simplified implementation we will use much smaller (number of units) `Dense` layers. Since the encoder block repeats, we have to be careful about the number of units in the `Dense` layer because the output dimension have to be compatible with the input for the next `MultiHeadAttention` layer.     \n",
    "\n",
    "**_Norm:_** The Norm in the figure refers to [`LayerNormalization` layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization). An excellent visual representation is available in the original [Group Normalization paper](https://arxiv.org/abs/1803.08494). In short, if we consider an input tensor with shape (N, C, H, W), It computes mean and variance $(\\mu_i, \\sigma _i)$ along the (C, H, W) axes. This ensures that computation for an input feature is entirely independent of other input features in a batch.  \n",
    "\n",
    "\n",
    "With these info we are ready to move!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1644144707447,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "DsR8uHmhFOw4",
    "outputId": "adc438aa-50f7-4b06-fefe-91a99788f962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape of Encoder block when inputs are the embeddings:  (1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# class mlp_block(layers.Layer):\n",
    "#   def __init__(self, mlp_dim):\n",
    "#     super(mlp_block, self).__init__()\n",
    "#     self.mlp_dim = mlp_dim\n",
    "#   def call(self, inputs):\n",
    "#     x = layers.Dense(units=self.mlp_dim, activation=tf.nn.gelu)(inputs)\n",
    "#     x = layers.Dropout(rate=0.1)(x) # dropout rate is from original paper,\n",
    "#     x = layers.Dense(units=inputs.shape[-1], activation=tf.nn.gelu)(x)\n",
    "#     x = layers.Dropout(rate=0.1)(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "def mlp_block_f(mlp_dim, inputs):\n",
    "  x = layers.Dense(units=mlp_dim, activation=tf.nn.gelu)(inputs)\n",
    "  x = layers.Dropout(rate=0.1)(x) # dropout rate is from original paper,\n",
    "  x = layers.Dense(units=inputs.shape[-1], activation=tf.nn.gelu)(x)\n",
    "  x = layers.Dropout(rate=0.1)(x)\n",
    "  return x\n",
    "\n",
    "# class Encoder1Dblock(layers.Layer):\n",
    "#   def __init__(self, num_heads, mlp_dim):\n",
    "#     super(Encoder1Dblock, self).__init__()\n",
    "#     self.num_heads = num_heads # number of heads in multi-head block\n",
    "#     self.mlp_dim = mlp_dim\n",
    "#   def call(self, inputs):\n",
    "#     x = layers.LayerNormalization(dtype=self.dtype)(inputs)\n",
    "#     x = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=inputs.shape[-1], dropout=0.1)(x, x) # self attention multi-head, dropout_rate is from original implementation\n",
    "#     x = layers.Add()([x, inputs]) # 1st residual part \n",
    "    \n",
    "    # #### mlp block \n",
    "    # y = layers.LayerNormalization(dtype=self.dtype)(x)\n",
    "    # y = mlp_block(mlp_dim=self.mlp_dim)(y)\n",
    "    # y_1 = layers.Add()([y, x]) #2nd residual part \n",
    "    # return y_1\n",
    "\n",
    "\n",
    "def Encoder1Dblock_f(num_heads, mlp_dim, inputs):\n",
    "  x = layers.LayerNormalization(dtype=inputs.dtype)(inputs)\n",
    "  x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1], dropout=0.1)(x, x) # self attention multi-head, dropout_rate is from original implementation\n",
    "  x = layers.Add()([x, inputs]) # 1st residual part \n",
    "  \n",
    "  y = layers.LayerNormalization(dtype=x.dtype)(x)\n",
    "  y = mlp_block_f(mlp_dim, y)\n",
    "  y_1 = layers.Add()([y, x]) #2nd residual part \n",
    "  return y_1\n",
    "\n",
    "\n",
    "#########################################\n",
    "# test with the embeddings as input\n",
    "#########################################\n",
    "# Encoder1Dblock_layer = Encoder1Dblock(num_heads=4, mlp_dim=32)\n",
    "# Encoder1Dblock_layer_out = Encoder1Dblock_layer(embedded_inp)\n",
    "Encoder1Dblock_layer_out_f = Encoder1Dblock_f(4, 32, embedded_inp)\n",
    "\n",
    "# print ('output shape of Encoder block when inputs are the embeddings: ', Encoder1Dblock_layer_out.shape)\n",
    "print ('output shape of Encoder block when inputs are the embeddings: ', Encoder1Dblock_layer_out_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFjxJ3xuCQSR"
   },
   "source": [
    "One final point, before combining all these for building the complete ViT, is the usage of `cls` token in the classfication part. This `cls` token is used as image representation and the output of this token is then transformed into a class prediction via a small multi-layer perceptron (MLP) with $\\textrm{tanh}$ as non-linearity in the single hidden layer. \n",
    "\n",
    "But this `cls` token isn't necessary. To quote from the ViT paper\n",
    "\n",
    ">This design is inherited from the Transformer model for text, and we use it throughout the main paper. An initial attempt at using only image-patch embeddings, global average-pooling (GAP) them, followed by a linear classifier—just like ResNet’s final feature map—performed very poorly.\n",
    "However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate. \n",
    "\n",
    "Due to this I will omit using the `cls` token and simply perform the Global Average Pooling (GAP) on the output of the transformer block which will be used as representation of the image. Finally we do the classification using an MLP and categorical cross entropy loss. \n",
    "\n",
    "We will add augmentation as layers and follow the generic `tf.data` pipeline which I've discussed in detail [here](https://towardsdatascience.com/time-to-choose-tensorflow-data-over-imagedatagenerator-215e594f2435). \n",
    "\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "## 3. Build ViT (Model and Model Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z5FvRGxx0QYs"
   },
   "outputs": [],
   "source": [
    "# explicitly check the implementation with 2 transformer encoder blocks\n",
    "\n",
    "# def build_ViT_Param():\n",
    "#   inputs = layers.Input(shape=train_im.shape[1:])\n",
    "#   # rescaling (normalizing pixel val between 0 and 1)\n",
    "#   rescale = rescale_layer(inputs)\n",
    "#   # augmentation \n",
    "#   augmented = data_augmentation(rescale)\n",
    "#   # generate patches with conv layer\n",
    "#   patches = generate_patch_conv_orgPaper(patch_size=patch_size, hidden_size=hidden_size)(augmented)\n",
    "#   embeddings = AddPositionEmbs(posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02))(patches)\n",
    "#   embeddings = layers.Dropout(rate=0.4)(embeddings)\n",
    "\n",
    "#   ######################################\n",
    "#   # ready for the transformer blocks\n",
    "#   # block 1\n",
    "#   ######################################\n",
    "\n",
    "#   ###############################\n",
    "#   # attention block 1\n",
    "#   ##############################\n",
    "\n",
    "#   x_1 = layers.LayerNormalization(dtype=embeddings.dtype)(embeddings)\n",
    "#   x_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1], dropout=0.1)(x_1, x_1) # self attention multi-head, dropout_rate is from original implementation\n",
    "#   x_1 = layers.Add()([x_1, embeddings]) # 1st residual part \n",
    "    \n",
    "#   #### mlp block 1\n",
    "#   y_1 = layers.LayerNormalization(dtype=x_1.dtype)(x_1)\n",
    "#   x_2 = layers.Dense(units=mlp_dim, activation='relu')(y_1)\n",
    "#   x_2 = layers.Dropout(rate=0.1)(x_2) # dropout rate is from original paper,\n",
    "#   x_2 = layers.Dense(units=y_1.shape[-1], activation=tf.nn.gelu)(x_2)\n",
    "#   x_2 = layers.Dropout(rate=0.1)(x_2)\n",
    "#   y_2 = layers.Add()([x_1, x_2]) #2nd residual part\n",
    "\n",
    "#   # ######################################\n",
    "#   # # ready for the transformer blocks\n",
    "#   # # block 2\n",
    "#   # ######################################\n",
    "\n",
    "#   # ###############################\n",
    "#   # # attention block 2\n",
    "#   # ##############################\n",
    "\n",
    "#   x_3 = layers.LayerNormalization(dtype=y_2.dtype)(y_2)\n",
    "#   x_3 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1], dropout=0.1)(x_3, x_3) # self attention multi-head, dropout_rate is from original implementation\n",
    "#   x_3 = layers.Add()([x_3, y_2]) # 1st residual part \n",
    "    \n",
    "#   # #### mlp block 2\n",
    "#   y_3 = layers.LayerNormalization(dtype=x_2.dtype)(x_3)\n",
    "#   x_4 = layers.Dense(units=mlp_dim, activation=tf.nn.gelu)(y_3)\n",
    "#   x_4 = layers.Dropout(rate=0.1)(x_4) # dropout rate is from original paper,\n",
    "#   x_4 = layers.Dense(units=y_3.shape[-1], activation=tf.nn.gelu)(x_4)\n",
    "#   x_4 = layers.Dropout(rate=0.1)(x_4)\n",
    "#   y_4 = layers.Add()([x_3, x_4]) #2nd residual part\n",
    "\n",
    "#   encoder_out = layers.LayerNormalization()(y_4)  \n",
    "\n",
    "#   # #####################################\n",
    "#   # #  final part (mlp to classification)\n",
    "#   # #####################################\n",
    "#   # #encoder_out_rank = int(tf.experimental.numpy.ndim(encoder_out))\n",
    "#   im_representation = tf.reduce_mean(encoder_out, axis=1)  # (1,) or (1,2)\n",
    "\n",
    "#   logits = layers.Dense(units=len(class_types), name='head', kernel_initializer=tf.keras.initializers.zeros)(im_representation) # !!! important !!! activation is linear \n",
    "\n",
    "#   final_model = tf.keras.Model(inputs = inputs, outputs = logits)\n",
    "\n",
    "#   return final_model\n",
    "\n",
    "\n",
    "# vit_model_params = build_ViT_Param()\n",
    "# vit_model_params.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2I3rAR-NSNfT"
   },
   "outputs": [],
   "source": [
    "### augment train but not test \n",
    "\n",
    "rescale_layer = tf.keras.Sequential([layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2), \n",
    "  layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),\n",
    "  layers.experimental.preprocessing.RandomTranslation(0.3, 0.3, fill_mode='reflect', interpolation='bilinear',)\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = (training_data.shuffle(40000).batch(128).map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=autotune).prefetch(autotune))\n",
    "valid_ds = (validation_data.shuffle(10000).batch(32).prefetch(autotune))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiE2g2ntiyqI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1384,
     "status": "ok",
     "timestamp": 1644144734034,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "4itznfBcnWS0",
    "outputId": "476c979f-cf96-41b4-8194-6ffc5d84a0fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 32, 32, 3)    0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 8, 8, 64)     3136        ['sequential[0][0]']             \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 64, 64)       0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " posembed_input (AddPositionEmb  (None, 64, 64)      4096        ['tf.reshape[0][0]']             \n",
      " s)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64, 64)       0           ['posembed_input[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 64, 64)      128         ['dropout_3[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 64, 64)      66368       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 64)       0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 64, 64)      128         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64, 128)      8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64, 128)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64, 64)       8256        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64, 64)       0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64, 64)       0           ['dropout_5[0][0]',              \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 64, 64)      128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 64, 64)      66368       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 64, 64)       0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 64, 64)      128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64, 128)      8320        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64, 128)      0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64, 64)       8256        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64, 64)       0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 64, 64)       0           ['dropout_7[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 64, 64)      128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 64, 64)      66368       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 64, 64)       0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 64, 64)      128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64, 128)      8320        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64, 128)      0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64, 64)       8256        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 64, 64)       0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 64, 64)       0           ['dropout_9[0][0]',              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 64, 64)      128         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 64, 64)      66368       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 64, 64)       0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 64, 64)      128         ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 64, 128)      8320        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 64, 128)      0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64, 64)       8256        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 64, 64)       0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 64, 64)       0           ['dropout_11[0][0]',             \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 64, 64)      128         ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 64, 64)      66368       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 64, 64)       0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 64, 64)      128         ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64, 128)      8320        ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 64, 128)      0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 64, 64)       8256        ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 64, 64)       0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 64, 64)       0           ['dropout_13[0][0]',             \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 64, 64)      128         ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 64, 64)      66368       ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 64, 64)       0           ['multi_head_attention_6[0][0]', \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 64, 64)      128         ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 64, 128)      8320        ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 64, 128)      0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 64, 64)       8256        ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 64, 64)       0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 64, 64)       0           ['dropout_15[0][0]',             \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " encoder_norm (LayerNormalizati  (None, 64, 64)      128         ['add_13[0][0]']                 \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 64)          0           ['encoder_norm[0][0]']           \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " head (Dense)                   (None, 10)           650         ['tf.math.reduce_mean[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 507,210\n",
      "Trainable params: 507,210\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# class Encoder(layers.Layer):\n",
    "#   def __init__(self, num_layers, mlp_dim, num_heads):\n",
    "#     super(Encoder, self).__init__()\n",
    "#     self.num_layers = num_layers # number of times encoder repeats\n",
    "#     self.mlp_dim = mlp_dim\n",
    "#     self.num_heads = num_heads\n",
    "\n",
    "#   def call(self, inputs):\n",
    "#     x = AddPositionEmbs(posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02), name='posembed_input')(inputs)\n",
    "#     x = layers.Dropout(rate=0.2)(x)\n",
    "#     for _ in range(self.num_layers):\n",
    "#       x = Encoder1Dblock(mlp_dim=self.mlp_dim, num_heads=self.num_heads)(x)\n",
    "\n",
    "#     encoded = layers.LayerNormalization(name='encoder_norm')(x)\n",
    "#     return encoded\n",
    "\n",
    "\n",
    "def Encoder_f(num_layers, mlp_dim, num_heads, inputs):\n",
    "  x = AddPositionEmbs(posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02), name='posembed_input')(inputs)\n",
    "  x = layers.Dropout(rate=0.2)(x)\n",
    "  for _ in range(num_layers):\n",
    "    x = Encoder1Dblock_f(num_heads, mlp_dim, x)\n",
    "\n",
    "  encoded = layers.LayerNormalization(name='encoder_norm')(x)\n",
    "  return encoded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# hyperparameter section \n",
    "###################################### \n",
    "transformer_layers = 6\n",
    "patch_size = 4\n",
    "hidden_size = 64\n",
    "num_heads = 4\n",
    "mlp_dim = 128\n",
    "\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_ViT():\n",
    "  inputs = layers.Input(shape=train_im.shape[1:])\n",
    "  # rescaling (normalizing pixel val between 0 and 1)\n",
    "  rescale = rescale_layer(inputs)\n",
    "  # augmentation \n",
    "  # augmented = data_augmentation(rescale)\n",
    "  # generate patches with conv layer\n",
    "  patches = generate_patch_conv_orgPaper_f(patch_size, hidden_size, rescale)\n",
    "\n",
    "  ######################################\n",
    "  # ready for the transformer blocks\n",
    "  ######################################\n",
    "  encoder_out = Encoder_f(transformer_layers, mlp_dim, num_heads, patches)  \n",
    "\n",
    "  #####################################\n",
    "  #  final part (mlp to classification)\n",
    "  #####################################\n",
    "  #encoder_out_rank = int(tf.experimental.numpy.ndim(encoder_out))\n",
    "  im_representation = tf.reduce_mean(encoder_out, axis=1)  # (1,) or (1,2)\n",
    "\n",
    "  logits = layers.Dense(units=len(class_types), name='head', kernel_initializer=tf.keras.initializers.zeros)(im_representation) # !!! important !!! activation is linear \n",
    "\n",
    "  final_model = tf.keras.Model(inputs = inputs, outputs = logits)\n",
    "  return final_model\n",
    "\n",
    "\n",
    "\n",
    "ViT_model = build_ViT()\n",
    "ViT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2537,
     "status": "ok",
     "timestamp": 1644144740615,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "506pV3ly8GOm",
    "outputId": "027148c0-3255-4eb4-a35c-860c8b014e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(ViT_model, rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55cWBC4P87oY"
   },
   "source": [
    "## 3.1. Model Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7909920,
     "status": "ok",
     "timestamp": 1644152655568,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "1GtczmxbrMsN",
    "outputId": "1de99926-9224-452e-c45f-613bb7280cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      " 86/313 [=======>......................] - ETA: 2:41 - loss: 2.1451 - accuracy: 0.1793 - top5 acc: 0.6953"
     ]
    }
   ],
   "source": [
    "### model \n",
    "\n",
    "ViT_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-3), \n",
    "                  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"), tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5 acc')]) \n",
    "#tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\")],) \n",
    "# from logits = True, because Dense layer has linear activation\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8,\n",
    "                              patience=5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "\n",
    "ViT_Train = ViT_model.fit(train_ds, \n",
    "                        epochs = 120, \n",
    "                        validation_data=valid_ds, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 2304,
     "status": "ok",
     "timestamp": 1644153046402,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "f6a5dQEUE_bu",
    "outputId": "2a831c86-28dd-4841-b948-038fc881c4d6"
   },
   "outputs": [],
   "source": [
    "### Plot train and validation curves\n",
    "loss = ViT_Train.history['loss']\n",
    "v_loss = ViT_Train.history['val_loss']\n",
    "\n",
    "acc = ViT_Train.history['accuracy'] \n",
    "v_acc = ViT_Train.history['val_accuracy']\n",
    "\n",
    "top5_acc = ViT_Train.history['top5 acc']\n",
    "val_top5_acc = ViT_Train.history['val_top5 acc']\n",
    "epochs = range(len(loss))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.yscale('log')\n",
    "plt.plot(epochs, loss, linestyle='--', linewidth=3, color='orange', alpha=0.7, label='Train Loss')\n",
    "plt.plot(epochs, v_loss, linestyle='-.', linewidth=2, color='lime', alpha=0.8, label='Valid Loss')\n",
    "# plt.ylim(0.3, 100)\n",
    "plt.xlabel('Epochs', fontsize=11)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, acc, linestyle='--', linewidth=3, color='orange', alpha=0.7, label='Train Acc')\n",
    "plt.plot(epochs, v_acc, linestyle='-.', linewidth=2, color='lime', alpha=0.8, label='Valid Acc') \n",
    "plt.xlabel('Epochs', fontsize=11)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, top5_acc, linestyle='--', linewidth=3, color='orange', alpha=0.7, label='Train Top 5 Acc')\n",
    "plt.plot(epochs, val_top5_acc, linestyle='-.', linewidth=2, color='lime', alpha=0.8, label='Valid Top5 Acc') \n",
    "plt.xlabel('Epochs', fontsize=11)\n",
    "plt.ylabel('Top5 Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('/content/gdrive/My Drive/Colab Notebooks/resnet/train_acc.png', dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cB9myn-JBVS"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def conf_matrix(predictions): \n",
    "    ''' Plots conf. matrix and classification report '''\n",
    "    cm=confusion_matrix(y_test, np.argmax(np.round(predictions), axis=1))\n",
    "    print(\"Classification Report:\\n\")\n",
    "    cr=classification_report(y_test,\n",
    "                                np.argmax(np.round(predictions), axis=1), \n",
    "                                target_names=[class_types[i] for i in range(len(class_types))])\n",
    "    print(cr)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    sns_hmp = sns.heatmap(cm, annot=True, xticklabels = [class_types[i] for i in range(len(class_types))], \n",
    "                yticklabels = [class_types[i] for i in range(len(class_types))], fmt=\"d\")\n",
    "    fig = sns_hmp.get_figure()\n",
    "    # fig.savefig('/content/gdrive/My Drive/Colab Notebooks/resnet/heatmap.png', dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8702,
     "status": "ok",
     "timestamp": 1644152778123,
     "user": {
      "displayName": "Swap vi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01936573407644251994"
     },
     "user_tz": -540
    },
    "id": "QkMZ50C7JfsB",
    "outputId": "83d9d1b7-1870-4afb-dc93-4d27122cd87f"
   },
   "outputs": [],
   "source": [
    "pred_class_resnet50 = ViT_model.predict(x_test)\n",
    "\n",
    "conf_matrix(pred_class_resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQPF2clcJzcP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNxuQ9acEiQ/1mCMRpLMClm",
   "collapsed_sections": [],
   "name": "Understand&Implement_VIT_TensorFlow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
